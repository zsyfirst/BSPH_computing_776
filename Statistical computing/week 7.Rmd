---
title: "Week 7"
author: "Siyu Zou"
date: "2023-10-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(forcats)
library(tidyverse)
```

# factor
At the end of this lesson you will:

How to create factors and some challenges working with them in base R
An introduction to the *forcats* package in the *tidyverse* to work with categorical variables in R

Factors are used for working with categorical variables, or variables that have a fixed and known set of possible values (income bracket, U.S. state, political affiliation).

Factors are useful when:

* You want to include categorical variables in regression models
* You want to plot categorical data (e.g. want to map categorical variables to aesthetic attributes)
* You want to display character vectors in a non-alphabetical order



## Factor basics
You can fix both of these problems with a factor.

To create a factor you must start by creating a list of the valid levels:
```{r factor creat a level}
month_levels <- c(
    "Jan", "Feb", "Mar", "Apr", "May", "Jun",
    "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"
)
```
Now we can create a factor with the factor() function defining the levels argument:
```{r factor add level}
x <- c("Dec", "Apr", "Jan", "Mar")
sort(x)
y <- factor(x, levels = month_levels)
y
```
We can see what happens if we try to sort the factor:
```{r}
sort(y)
```
We can also check the attributes of the factor:
```{r}
attributes(y)
```

If you want to access the set of levels directly, you can do so with levels():
```{r}
levels(y)
```

Any values not in the level will be silently converted to NA:
```{r typo NA}
x_typo <- c("DEC", "dec", "jan")
y_typo <- factor(x_typo, levels = month_levels)
y_typo
```

## Challenges working with categorical data
```{r}
library(tidyverse)

x1_original <- c(10, 10, 10, 50, 60, 20, 20, 40)
x1_factor <- factor(x1_original)
attributes(x1_factor)
```

```{r tibble}
tibble(x1_original, x1_factor) %>%
    mutate(x1_numeric = as.numeric(x1_factor))
```
Instead of creating a new variable with a numeric version of the value of the factor variable x1_factor, the variable loses the original numerical categories and creates a factor number (i.e., 10 is mapped to 1, 20 is mapped to 2, and 40 is mapped to 3, etc).

This result is unexpected because base::as.numeric() is intended to recover numeric information by coercing a character variable.




The tidyverse is part of this movement, with functions from the readr package defaulting to leaving strings as-is. Others used to chose to add options(stringAsFactors = FALSE) into their start up commands to override R’s default of stringsAsFactors = TRUE in functions such as read.table(). However, that is no longer needed in recent versions of R as the default has become stringsAsFactors = FALSE as documented on the official R blog: https://blog.r-project.org/2020/02/16/stringsasfactors/.




## Factors when modeling data
Factors are still necessary for some data analytic tasks. The most salient case is in statistical modeling.
When you pass a factor variable into lm() or glm(), R automatically creates indicator (or more colloquially *‘dummy’)* variables for each of the levels and picks one as a reference group.

For simple cases, this behavior can also be achieved with a character vector.

However, to choose *which level to use as a reference level* or to order classes, factors must be used.

Consider a vector of character strings with three income levels:
```{r}
income_level <- c(
    rep("low", 10),
    rep("medium", 10),
    rep("high", 10)
)
income_level
```

Here, it might make sense to use the lowest income level (low) as the reference class so that all the other coefficients can be interpreted in comparison to it.

However, R would use high as the reference by default because ‘h’ comes before ‘l’ in the alphabet.

```{r}
x <- factor(income_level)
x
```

```{r}
y <- rnorm(30) # generate some random obs from a normal dist
lm(y ~ x)  # high is the ref

x <- relevel(x, "low")
lm(y ~ x)  # low is the ref
```


### Memory req for factors and character strings

In early versions of R, storing categorical data as a factor variable was considerably more efficient than storing the same data as strings, because factor variables only store the factor labels once.
However, R now uses a global string pool, so each unique string is only stored once, which means storage is now less of an issue.
```{r}
income_level <- c(
    rep("low", 10000),
    rep("medium", 10000),
    rep("high", 10000)
)

format(object.size(income_level), units = "Kb") # size of the character string

format(object.size(factor(income_level)), units = "Kb") # size of the factor

```



## forcats
Next, we will introduce the forcats package, which is part of the core tidyverse, but can also be loaded directly

library("forcats")

It provides tools for dealing with *cat*egorical variables (and it’s an anagram of factors!) using a wide range of helpers for working with factors.

### Example: 
For the rest of this lecture, we are going to use the gss_cat dataset that is installed when you load forcats.
```{r}
gss_cat
```

When factors are stored in a tibble, you cannot see their levels so easily. One way to view them is with count():
```{r}
gss_cat %>%
    count(race)

gss_cat %>%
    ggplot(aes(x = race)) +
    geom_bar()
```
When working with factors, the two most common operations are

Changing the order of the levels
Changing the values of the levels


### Modifying factor order
It’s often useful to change the order of the factor levels in a visualization.

Let’s explore the *relig* (religion) factor
```{r modify order}
gss_cat %>%
    count(relig)

attributes(gss_cat$relig)
```

The first level is “No answer” followed by “Don’t know”, and so on.

Imagine you want to explore the average number of hours spent watching TV (*tvhours*) per day across religions (*relig*):
```{r}
relig_summary <- gss_cat %>%
    group_by(relig) %>%
    summarise(
        tvhours = mean(tvhours, na.rm = TRUE),
        n = n()
    )

relig_summary %>%
    ggplot(aes(x = tvhours, y = relig)) +
    geom_point()
```
The y-axis lists the levels of the relig factor in the order of the levels.

However, it is hard to read this plot because there’s no overall pattern.


## fct_reorder
We can improve it by reordering the levels of relig using fct_reorder(). 
fct_reorder(.f, .x, .fun) takes three arguments:
* .f, the factor whose levels you want to modify.
* .x, a numeric vector that you want to use to reorder the levels.
* Optionally, .fun, a function that’s used if there are multiple values of x for each value of f. The default value is median.

```{r}
relig_summary %>%
    ggplot(aes(
        x = tvhours,
        y = fct_reorder(.f = relig, .x = tvhours)
    )) +
    geom_point()
```
Reordering religion makes it much easier to see that people in the “Don’t know” category watch much more TV, and Hinduism & Other Eastern religions watch much less.

As you start making more complicated transformations, I recommend moving them out of aes() and into a separate mutate() step.


Example
You could rewrite the plot above as:
```{r}
relig_summary %>%
    mutate(relig = fct_reorder(relig, tvhours)) %>%
    ggplot(aes(x = tvhours, y = relig)) +
    geom_point()
```

Another example
What if we create a similar plot looking at how average age varies across reported income level?
```{r}
rincome_summary <-
    gss_cat %>%
    group_by(rincome) %>%
    summarise(
        age = mean(age, na.rm = TRUE),
        n = n()
    )

## Original rincome order
rincome_summary %>%
    ggplot(aes(x = age, y = rincome)) +
    geom_point()
```

## rincome re-ordered by age's values
```{r}
rincome_summary %>%
    ggplot(aes(x = age, y = fct_reorder(.f = rincome, .x = age))) +
    geom_point()

```

Reserve fct_reorder() for factors whose levels are arbitrarily ordered.


#### Question
Let’s practice fct_reorder(). Using the palmerpenguins dataset,

Calculate the average bill_length_mm for each species
Create a scatter plot showing the average for each species.
Go back and reorder the factor species based on the average bill length from largest to smallest.
Now order it from smallest to largest
```{r}
library(palmerpenguins)
penguins

bill_length_summary <- penguins %>%
  group_by(species) %>%
  summarise(
    mean_bill_length_mm = mean(bill_length_mm, na.rm = TRUE),
    n = n()
  )

bill_length_summary %>%
  mutate(species = fct_reorder(species,mean_bill_length_mm ) ) %>%
  ggplot(aes(x = mean_bill_length_mm, y = species ))  + 
    geom_point()
  
```

### fct_relevel
However, it does make sense to pull “Not applicable” to the front with the other special levels.

You can use fct_relevel().

It takes a factor, f, and then any number of levels that you want to move to the front of the line.
```{r}
rincome_summary %>%
    ggplot(aes(age, fct_relevel(rincome, "Not applicable"))) +
    geom_point()
```

Any levels not mentioned in fct_relevel will be left in their existing order.

Another type of reordering is useful when you are coloring the lines on a plot. *fct_reorder2(f, x, y)* reorders the factor f by the y values associated with the largest x values.
```{r}
by_age <-
    gss_cat %>%
    filter(!is.na(age)) %>%
    count(age, marital) %>%
    group_by(age) %>%
    mutate(prop = n / sum(n))

by_age %>%
    ggplot(aes(age, prop, colour = marital)) +
    geom_line(na.rm = TRUE)

by_age %>%
    ggplot(aes(age, prop, colour = fct_reorder2(marital, age, prop))) +
    geom_line() +
    labs(colour = "marital")

```


### fct_infreq
Finally, for bar plots, you can use fct_infreq() to order levels in decreasing frequency: this is the simplest type of reordering because it doesn’t need any extra variables. Combine it with *fct_rev()* if you want them in increasing frequency so that in the bar plot largest values are on the right, not the left.
```{r order in frequency}
gss_cat %>%
    mutate(marital = marital %>% fct_infreq() %>% fct_rev()) %>%
    ggplot(aes(marital)) +
    geom_bar()
```

```{r}
fct_rev(x$species)
# same to this: 
levels(x$species) <- rev(x$species)
```

## Modifying factor levels
#### fct_recode
The most general and powerful tool is fct_recode(). It allows you to recode, or change, the value of each level. For example, take the gss_cat$partyid:
```{r fct_recode}
gss_cat %>%
    count(partyid)
```

The levels are terse and inconsistent.

Let’s tweak them to be longer and use a parallel construction.

Like most rename and recoding functions in the tidyverse:

* *the new values go on the left*
* the old values go on the right

```{r}
gss_cat %>%
    mutate(partyid = fct_recode(partyid,
        "Republican, strong"    = "Strong republican",
        "Republican, weak"      = "Not str republican",
        "Independent, near rep" = "Ind,near rep",
        "Independent, near dem" = "Ind,near dem",
        "Democrat, weak"        = "Not str democrat",
        "Democrat, strong"      = "Strong democrat"
    )) %>%
    count(partyid)


```

fct_recode() will leave the levels that aren’t explicitly mentioned as is, and will warn you if you accidentally refer to a level that doesn’t exist.

#### combine 
To combine groups, you can assign multiple old levels to the same new level:
```{r}
gss_cat %>%
    mutate(partyid = fct_recode(partyid,
        "Republican, strong"    = "Strong republican",
        "Republican, weak"      = "Not str republican",
        "Independent, near rep" = "Ind,near rep",
        "Independent, near dem" = "Ind,near dem",
        "Democrat, weak"        = "Not str democrat",
        "Democrat, strong"      = "Strong democrat",
        "Other"                 = "No answer",
        "Other"                 = "Don't know",
        "Other"                 = "Other party"
    )) %>%
    count(partyid)
```


### fct_collapse
If you want to collapse a lot of levels, fct_collapse() is a useful variant of fct_recode().
For each new variable, you can provide a vector of old levels:
```{R}
gss_cat %>%
    mutate(partyid = fct_collapse(partyid,
        "other" = c("No answer", "Don't know", "Other party"),
        "rep" = c("Strong republican", "Not str republican"),
        "ind" = c("Ind,near rep", "Independent", "Ind,near dem"),
        "dem" = c("Not str democrat", "Strong democrat")
    )) %>%
    count(partyid)
```

#### fct_lump_*
Sometimes you just want to lump together the small groups to make a plot or table simpler.

That’s the job of the fct_lump_*() family of functions.

fct_lump_lowfreq() is a simple starting point that progressively lumps the smallest groups categories into “Other”, always keeping “Other” as the smallest category.
```{r}
gss_cat %>%
    mutate(relig = fct_lump_lowfreq(relig)) %>%
    count(relig)
```

In this case it’s not very helpful: it is true that the majority of Americans in this survey are Protestant, but we’d probably like to see some more details!
Instead, we can use the *fct_lump_n()* to specify that we want exactly 10 groups:

```{r}
gss_cat %>%
    mutate(relig = fct_lump_n(relig, n = 10)) %>%
    count(relig, sort = TRUE) %>%
    print(n = Inf)
```
Read the documentation to learn about fct_lump_min() and fct_lump_prop() which are useful in other cases.


## Ordered factors
There’s a special type of factor that needs to be mentioned briefly: ordered factors.
Ordered factors, created with ordered(), imply a strict ordering and equal distance between levels:

The first level is “less than” the second level by the same amount that the second level is “less than” the third level, and so on…
```{r}
ordered(c("a", "b", "c"))

factor(c("a","b","c"))

model.matrix(~factor(c("a","b","c")))
model.matrix(~ordered(c("a","b","c")))
```
However, in practice, ordered() factors behave very similarly to regular factors.





# Tidytext and sentiment analysis
In this lecture, we will be asking the following questions:

Which are the most commonly used words from Jane Austen’s novels?
Which are the most positive or negative words?
How does the sentiment (e.g. positive vs negative) of the text change across each novel?


To answer these questions, we will need to learn about a few things. Specifically,

* 1. How to convert words in documents to a tidy text format using the tidytext R package.
* 2. A little bit about sentiment analysis.

## Tidy text
In previous lectures, you have learned about the tidy data principles and the *tidyverse* R packages as a way to make handling data easier and more effective.

These packages depend on *data being formatted in a particular way*.

The idea with tidy text is to *treat text as data frames of individual words* and *apply the same tidy data principles* to make text mining tasks easier and consistent with already developed tools.


### What is a tidy format?
Another way of putting it is that it is a set of packages that are useful specifically for data manipulation, exploration and visualization with a common philosophy.

#### What is this common philosphy?
The common philosophy is called “tidy” data.

In tidy data:

*Each variable forms a column.
*Each observation forms a row.
*Each type of observational unit forms a table.

#### What is a tidy text format?
When dealing with *text* data, the *tidy text* format is defined as a table *with one-token-per-row*, where a *token* is a meaningful unit of text (e.g. a word, pair of words, sentence, paragraph, etc).

Using a *given set of token*, we can *tokenize* text, or *split the text into the defined tokens of interest along the rows*.

In contrast, other data structures that are commonly used to store text data in text mining applications:
* string: text can, of course, be stored as strings, i.e., character vectors, within R, and often text data is first read into memory in this form.
* corpus: these types of objects typically contain raw strings annotated with additional metadata and details.
* document-term matrix: This is a sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count.


#### Why is this format useful?
One of the biggest advantages of transforming text data to the tidy text format is that it allows data to transition smoothly between other packages that adhere to the *tidyverse* framework (e.g. *ggplot2, dplyr*, etc).


## How does it work?
The main workhorse function in the tidytext R package to tokenize text a data frame is the *unnest_tokens*(tbl, output, input) function.
```{r unnest_tokens}
# install.packages("tidytext")
library(tidytext)
??unnest_tokens
```

In addition to the tibble or data frame (tbl), the function needs two basic arguments:

* output or the output column name that will be created (e.g. string) as the text is unnested into it
* input or input column name that the text comes from and gets split


## 
```{r}
library(tidyverse)
library(stringr)
library(tidytext) ## needs to be installed
library(janeaustenr) ## needs to be installed
```
```{r example}
peng_preface <-
    c(
        "I started using R in 1998 when I was a college undergraduate working on my senior thesis.",
        "The version was 0.63.",
        "I was an applied mathematics major with a statistics concentration and I was working with Dr. Nicolas Hengartner on an analysis of word frequencies in classic texts (Shakespeare, Milton, etc.).",
        "The idea was to see if we could identify the authorship of each of the texts based on how frequently they used certain words.",
        "We downloaded the data from Project Gutenberg and used some basic linear discriminant analysis for the modeling.",
        "The work was eventually published and was my first ever peer-reviewed publication.",
        "I guess you could argue it was my first real 'data science' experience."
    )

peng_preface
```

Then, we use the tibble() function to construct a data frame with two columns: one counting the line number and one from the character strings in peng_preface
```{r}
peng_preface_df <- tibble(
    line = 1:7,
    text = peng_preface
)
peng_preface_df
```

### Text Mining and Tokens
Next, we will use the unnest_tokens() function where we will call the output column to be created word and the input column text from the peng_preface_df.

```{r}
peng_token <-
    peng_preface_df %>%
    unnest_tokens(
        output = word,
        input = text,
        token = "words"
    )

peng_token %>%
    head()

# lower case
```

The argument token="words" defines the unit for tokenization.

The default is "words", but there are lots of other options.

```{r characters}
peng_preface_df %>%
    unnest_tokens(word,
        text,
        token = "characters"
    ) %>%
    head()
```

or something called *ngrams*, which is defined by Wikipedia as a “contiguous sequence of n items from a given sample of text or speech”
```{r ngrams}
peng_preface_df %>%
    unnest_tokens(word,
        text,
        token = "ngrams",
        n = 3
    ) %>%
    head()
```
Another option is to use the *character_shingles* option, which is similar to tokenizing like ngrams, except the units are characters instead of words.
```{r shingles}
peng_preface_df %>%
    unnest_tokens(word,
        text,
        token = "character_shingles",
        n = 4
    ) %>%
    head()
```

You can also *create custom functions* for tokenization
```{r use custom function}
peng_preface_df %>%
    unnest_tokens(word,
        text,
        token = stringr::str_split,
        pattern = " "
    ) %>%
    head()
```


### Question
Let’s tokenize the first four sentences of Amanda Gorman’s The Hill We Climb by words.

```{r practice}
gorman_hill_we_climb <-
    c(
        "When day comes we ask ourselves, where can we find light in this neverending shade?",
        "The loss we carry, a sea we must wade.",
        "We’ve braved the belly of the beast, we’ve learned that quiet isn’t always peace and the norms and notions of what just is, isn’t always justice.",
        "And yet the dawn is ours before we knew it, somehow we do it, somehow we’ve weathered and witnessed a nation that isn’t broken but simply unfinished."
    )

hill_df <- tibble(
    line = seq_along(gorman_hill_we_climb),
    text = gorman_hill_we_climb
)
hill_df
```

try it out
```{r}
hill_df %>%
    unnest_tokens(
        output = wordsforfun,
        input = text,
        token = "words"
    )
```


### Example: text from works of Jane Austen
We will use the text from six published novels from Jane Austen, which are available in the janeaustenr R package. The authors describe the format:
```{r}
library(janeaustenr)
head(prideprejudice, 20)
```
Similar to what we did above with Roger’s preface, we can

Turn the text of character strings into a data frame and then
Convert it into a one-row-per-line dataframe using the unnest_tokens() function

```{r}
pp_book_df <- tibble(text = prideprejudice)

pp_book_df %>%
    unnest_tokens(
        output = word,
        input = text,
        token = "words"
    )
```

We can also divide it by paragraphs
```{r paragraphs}
tmp <- pp_book_df %>%
    unnest_tokens(
        output = paragraph,
        input = text,
        token = "paragraphs"
    )
tmp
```
We can extract a particular element from the tibble
```{r}
tmp[3, 1]
```
What you name the output column, e.g. "paragraph" in this case, doesn’t affect it, it’s just good to give it a consistent name.

We could also divide it by sentence:
```{r}
pp_book_df %>%
    unnest_tokens(
        output = sentence,
        input = text,
        token = "sentences"
    )
```
This is tricked by terms like “Mr.” and “Mrs.”

One neat trick is that we can unnest by two layers:

paragraph and then
word
This lets us keep track of *which paragraph is which.*
```{r}
paragraphs <-
    pp_book_df %>%
    unnest_tokens(
        output = paragraph,
        input = text,
        token = "paragraphs"
    ) %>%
    mutate(paragraph_number = row_number())

paragraphs
```
We use mutate() to annotate a paragraph number quantity to keep track of paragraphs in the original format.

After tokenizing by paragraph, we can then tokenzie by word:
```{r}
paragraphs %>%
    unnest_tokens(
        output = word,
        input = paragraph
    )
```
We notice there are many what are called stop words (“the”, “of”, “to”, and so forth in English).

Often in text analysis, we will want to *remove stop words* because stop words are words that are not useful for an analysis.
```{r remove stop words}
data(stop_words)

table(stop_words$lexicon)
stop_words %>%
    head(n = 10)
```

We can remove stop words (kept in the *tidytext* dataset *stop_words*) with an *anti_join(x,y)* (return all rows from x without a match in y).
```{r}
words_by_paragraph <-
    paragraphs %>%
    unnest_tokens(
        output = word,
        input = paragraph
    ) %>%
    anti_join(stop_words)
```
```{r}
words_by_paragraph
```
Because we have stored our data in a tidy dataset, we can use tidyverse packages for exploratory data analysis.

For example, here we use *dplyr*’s  *count()* fcuntion to find the most common words in the book
```{r}
words_by_paragraph %>%
    count(word, sort = TRUE) %>%
    head()
```
hen use *ggplot2* to plot the most commonly used words from the book
```{r}
words_by_paragraph %>%
    count(word, sort = TRUE) %>%
    filter(n > 150) %>%
    mutate(word = fct_reorder(word, n)) %>%
    ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
```

We can also do this for all of her books using the *austen_books()* object
```{r}
austen_books() %>%
    head()
```

We can do some data wrangling that keep tracks of the line number and chapter (using a *regex*) to find where all the chapters are.
```{r}
original_books <-
    austen_books() %>%
    group_by(book) %>%
    mutate(
        linenumber = row_number(),
        chapter = cumsum(
            str_detect(text,
                pattern = regex(
                    pattern = "^chapter [\\divxlc]",
                    ignore_case = TRUE   # ignore upper case, lower case
                )
            )
        )
    ) %>%
    ungroup()

original_books
```

Finally, we can restructure it to a one-token-per-row format using the *unnest_tokens()* function and remove stop words using the *anti_join()* function in *dplyr.*

```{r}
tidy_books <- original_books %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_words)

tidy_books
```
Here are the most commonly used words across all of Jane Austen’s books.
```{r}
tidy_books %>%
    count(word, sort = TRUE) %>%
    filter(n > 600) %>%
    mutate(word = fct_reorder(word, n)) %>%
    ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
```

# Sentiment Analysis
In the previous section, we explored the tidy text format and showed how we can calculate things such as word frequency.
Next, we are going to look at something called *opinion mining* or *sentiment analysis*. The tidytext authors write:
Let’s try using sentiment analysis on the Jane Austen books.

## The sentiments dataset
Inside the *tidytext* package are several *sentiment lexicons*. A few things to note:

* The lexicons are based on unigrams (single words)
* The lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth

You can use the *get_sentiments()* function to extract a specific lexicon.

The *nrc* lexicon *categorizes words into multiple categories* of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust
```{r}
# install.packages("textdata")
library("textdata")
get_sentiments("nrc")
```

The bing lexicon categorizes words in a binary fashion into positive and negative categories
```{r}
get_sentiments("bing")
```

The *AFINN* lexicon assigns words with a score that runs between *-5 and 5*, with negative scores indicating negative sentiment and positive scores indicating positive sentiment
```{r}
get_sentiments("afinn")
```

Joining together tidy text data with lexicons
Now that we have our data in a tidy text format AND we have learned about different types of lexicons in application for sentiment analysis, we can *join the words together* using a join function.

What are the most common joy words in the book Emma?

Here, we use the *nrc* lexicon and join the *tidy_books* dataset with the *nrc_joy* lexicon using the *inner_join*() function in dplyr.

```{r}
nrc_joy <- get_sentiments("nrc") %>%
    filter(sentiment == "joy")

tidy_books %>%
    filter(book == "Emma") %>%
    inner_join(nrc_joy) %>%
    count(word, sort = TRUE)
```
We can do things like investigate how the sentiment of the text changes throughout each of Jane’s novels.

Here, we use the *bing* lexicon, find a sentiment score for each word, and then use inner_join().
```{r}
tidy_books %>%
    inner_join(get_sentiments("bing"))
```
Then, we can count how many positive and negative words there are in each section of the books.

We create an index to help us keep track of where we are in the narrative, which uses integer division, and counts up sections of 80 lines of text.
```{r}
tidy_books %>%
    inner_join(get_sentiments("bing")) %>%
    count(book,
        index = linenumber %/% 80,
        sentiment
    )
```

The %/% operator does integer division (x %/% y is equivalent to floor(x/y)) so the index keeps track of which 80-line section of text we are counting up negative and positive sentiment in.


Finally, we use *pivot_wider()* to have positive and negative counts in different columns, and then use mutate() to calculate a net sentiment (positive - negative).
```{r}
jane_austen_sentiment <-
    tidy_books %>%
    inner_join(get_sentiments("bing")) %>%
    count(book,
        index = linenumber %/% 80,
        sentiment
    ) %>%
    pivot_wider(
        names_from = sentiment,
        values_from = n,
        values_fill = 0
    ) %>%
    mutate(sentiment = positive - negative)

jane_austen_sentiment
```
Then we can plot the sentiment scores across the sections of each novel:
```{r}
jane_austen_sentiment %>%
    ggplot(aes(x = index, y = sentiment, fill = book)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(. ~ book, ncol = 2, scales = "free_x")
```


## Word clouds
You can also do things like create word clouds using the wordcloud package.
```{r}
# install.packages("wordcloud")
library(wordcloud)
```

```{r}
tidy_books %>%
    anti_join(stop_words) %>%
    count(word) %>%
    with(wordcloud(word, n, max.words = 100))
```


## Converting to and from tidy and non-tidy formats

In this section, we want to *convert our tidy text data* constructed with the unnest_tokens() function (useable by packages in the tidyverse) into a different format that can be *used by packages for natural language processing* or other types of machine learning algorithms in non-tidy formats.
A flowchart of a typical text analysis that combines tidytext with other tools and data formats, particularly the *tm* or *quanteda* packages. Here, we show how to convert back and forth between document-term matrices and tidy data frames, as well as converting from a Corpus object to a text data frame.






